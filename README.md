# pre-trained-language-models

- [bengio-NPLM] A Neural Probabilistic Language Model 

- [Mikolov-word2vec-models] Efficient Estimation of Word Representations in Vector Space

- [Mikolov-word2vec-training for skip-gram] Distributed Representations of Words and Phrases and their Compositionality

- [doc2vec] Distributed representations of sentences and documents

- ELMO

- GPT

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

- XLNet: Generalized Autoregressive Pretraining for Language Understanding
